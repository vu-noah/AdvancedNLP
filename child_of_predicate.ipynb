{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "650ee3d2-e21f-4406-9748-53cd4add3937",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (77824497.py, line 97)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [10]\u001b[1;36m\u001b[0m\n\u001b[1;33m    else:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# see 3) child of predicate\n",
    "\n",
    "\n",
    "\n",
    "# 21.02.2023\n",
    "# Noah-Manuel Michael\n",
    "# Advanced NLP Assignment 2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_features_to_determine_candidates(filepath):\n",
    "    \"\"\"\n",
    "    Extract features for determining whether a token is a SR candidate.\n",
    "    :param str filepath: the path to the preprocessed file\n",
    "    :return: zip object (categorical_feature_dicts, numerical_feature_dicts)\n",
    "    \"\"\"\n",
    "    # read in the tsv file (that has no header row), assign column names, and store the data in a pandas dataframe\n",
    "    df = pd.read_csv(filepath, sep='\\t', header=None, names=['token_individual_id', 'token_global_id',\n",
    "                                                             'token_id_in_sent', 'token', 'lemma',\n",
    "                                                             'UPOS', 'POS', 'grammar', 'head_id', 'dependency_label',\n",
    "                                                             'head_dependency_relation', 'additional_info',\n",
    "                                                             'PB_predicate', 'semantic_role', 'is_candidate', 'sent_id',\n",
    "                                                             'current_predicate', 'global_sent_id'],\n",
    "                     quotechar='Ä…', engine='python')  # by setting 'quotechar' to a letter that is not part of the tsv file, \n",
    "                                                      # we make sure that nothing is counted as a quotechar (to solve the errors with punctuation chars in italics) \n",
    "\n",
    "    # create two empty lists to put the feature dicts in later\n",
    "    categorical_feature_dicts = []\n",
    "    numerical_feature_dicts = []\n",
    "    \n",
    "    # create a dataframe for each sentence (i.e. rows with the same sent_id) in the same order as the original file \n",
    "    for group in df.groupby(\"sent_id\", sort = False):\n",
    "        sent_df = group[1]\n",
    "\n",
    "        # for each token in the sentence:  \n",
    "        for i, row in sent_df.iterrows():\n",
    "            \n",
    "            # create 2 dicts to store the categorical and numerical features in later\n",
    "            categorical_feature_dict = {}\n",
    "            numerical_feature_dict = {}\n",
    "\n",
    "            # 1) extract the lemma and POS of the current token\n",
    "            categorical_feature_dict['lemma'] = row['lemma']\n",
    "\n",
    "            categorical_feature_dict['UPOS'] = row['UPOS']\n",
    "            categorical_feature_dict['POS'] = row['POS']\n",
    "        \n",
    "            # 2) extract the lemma of the head of the current token\n",
    "            head_id = row['head_id']\n",
    "            if head_id.isdigit():\n",
    "                try:\n",
    "                    # find row(s) in the dataframe whose token id equals the current token's head id\n",
    "                    head_lemmas = sent_df.loc[sent_df['token_id_in_sent'] == int(head_id)]\n",
    "                    categorical_feature_dict['lemma_of_head'] = head_lemmas.iloc[0]['lemma']\n",
    "                # if the current token is the root, the above gives an IndexError; in that case we add 'None' to the\n",
    "                # feature dict\n",
    "                except IndexError:\n",
    "                    categorical_feature_dict['lemma_of_head'] = None\n",
    "            else:\n",
    "                categorical_feature_dict['lemma_of_head'] = head_id\n",
    "            \n",
    "            # extract whether the token is a NE (check whether UPOS is PROPN)\n",
    "            if row['UPOS'] == 'PROPN':\n",
    "                numerical_feature_dict['is_NE'] = 1\n",
    "            else:\n",
    "                numerical_feature_dict['is_NE'] = 0\n",
    "\n",
    "            print(categorical_feature_dict, numerical_feature_dict)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    " #3) extract whether the current token is a direct child of its predicate (code needs to be reviewed)\n",
    "    \n",
    "    # create a dataframe for each sentence copy (i.e. rows with the same global_sent_id) in the same order as the original file \n",
    "    for group in df.groupby(\"global_sent_id\", sort=False):\n",
    "        global_sent_df = group[1]\n",
    "\n",
    "        direct_child = {}\n",
    "\n",
    "        # for each token in the sentence copy: \n",
    "        for i, row in global_sent_df.iterrows():\n",
    "\n",
    "            #If the corresponding 'token_id_in_sent' matches the same value in the 'head_id' column of all rows in that sentence copy:                \n",
    "            if row['token_id_in_sent'] in global_sent_df['head_id'].values:\n",
    "\n",
    "                # if it has the same head_id as the predicate: this is the row with the value of in PB_predicate == the value of current_predicate in that row.\n",
    "                if row['head_id'] == global_sent_df.loc[global_sent_df['PB_predicate'] == global_sent_df.loc[global_sent_df['current_predicate']:\n",
    "                                                                                                             \n",
    "                    # set the value for 'is_direct_child' to 1\n",
    "                    direct_child['is_direct_child']  == 1\n",
    "                                                                                                             \n",
    "                # else, set the value for 'is_direct_child' to 0                                                                                            \n",
    "                else:\n",
    "                    direct_child['is_direct_child']  == 0\n",
    "                                                                                                            \n",
    "            else:\n",
    "                direct_child['is_direct_child']  == 0                                                                                        \n",
    "\n",
    "            print (direct_child)\n",
    "                                                                                                              \n",
    "            \n",
    "                                                                                                                                                                                                                                                                                                          \n",
    "                                                                                                             \n",
    "                                                                                                             \n",
    "            # append the feature dicts to the lists\n",
    "            categorical_feature_dicts.append(categorical_feature_dict)\n",
    "            numerical_feature_dicts.append(numerical_feature_dict)\n",
    "            numerical_feature_dicts.append(direct_child)\n",
    "\n",
    "    #return a zip with the two lists filled with feature dicts\n",
    "    return zip(categorical_feature_dicts, numerical_feature_dicts)\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
    "\n",
    "# extract the features to determine the candidates\n",
    "if __name__ == '__main__':\n",
    "    candidate_feature_dicts_train = extract_features_to_determine_candidates('Data/train_data.tsv')\n",
    "    candidate_feature_dicts_test = extract_features_to_determine_candidates('Data/test_data.tsv')\n",
    "\n",
    "    # test the code\n",
    "    for tup in candidate_feature_dicts_test:\n",
    "        print(tup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70df1de-db3a-46e1-b6e1-541f843622d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
